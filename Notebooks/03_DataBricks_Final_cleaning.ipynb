{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db19d42b-5f66-4a2d-9859-ea87029e54e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# commenting the access key in order to publish to github\n",
    "spark.conf.set(\n",
    "\"fs.azure.account.key.goodreadsreviews60104758.dfs.core.windows.net\",\n",
    "\"<access-key>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af156d5-3015-47f9-b464-a7b770f57183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+---------+------------------+--------------------+------+--------------------+-------------+-------+--------------------+\n",
      "|           review_id| book_id|               title|author_id|              name|             user_id|rating|         review_text|language_code|n_votes|          date_added|\n",
      "+--------------------+--------+--------------------+---------+------------------+--------------------+------+--------------------+-------------+-------+--------------------+\n",
      "|fa241e939d2218940...| 7663760|Fooling Some of t...|  5220763|      Azize Bergin|0c5b36407771dfd65...|     4|Investing is an o...|          eng|      0|Tue Apr 07 16:19:...|\n",
      "|2628b9004c4b6710a...|22387890|          Deep Water|  4986447|    Ilhan Yabantas|b78a9143ca2f0c4c7...|     3|I liked this stor...|          eng|      1|Sun Jun 01 21:34:...|\n",
      "|ec6f00823d47459dc...| 1096390| The Uncommon Reader|  4416306|Charlotte Bennardo|c3fa377cfc8440174...|     4|cute story with l...|          eng|      0|Tue Jan 05 08:20:...|\n",
      "|e995dd91ce98a4830...|13166894|Death at SeaWorld...|  4816880|     Peggy Laborde|ddc44923909c38b4d...|     4|This was a good b...|          eng|      0|Tue Jul 09 21:53:...|\n",
      "|d6120da6fdef79aaa...|25430624|           ABC Dream|  5022408|  Martine Kamphuis|419dfd723edeb5e27...|     5|Concept: 5 stars ...|             |      0|Sun Jan 01 23:17:...|\n",
      "+--------------------+--------+--------------------+---------+------------------+--------------------+------+--------------------+-------------+-------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Path to your curated Gold dataset\n",
    "# Test the connection\n",
    "gold_path_v1 = \"abfss://lakehouse@goodreadsreviews60104758.dfs.core.windows.net/Gold_layer/curated_reviews_delta\"\n",
    "df = spark.read.format(\"delta\").load(gold_path_v1)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc12ed4b-ed3f-4fe1-a770-73519191a52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- n_votes: long (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32f59ad-7391-4b50-8185-f8c81769e184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fixing the data types\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"rating\", df[\"rating\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"n_votes\", df[\"n_votes\"].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1fd1bf-e9d7-4262-8af6-edcfd2301b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- n_votes: integer (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2473b9-520e-4b71-bae3-1b79a5ab93f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_added                    |\n",
      "+------------------------------+\n",
      "|Tue Apr 07 16:19:43 -0700 2015|\n",
      "|Sun Jun 01 21:34:59 -0700 2014|\n",
      "|Tue Jan 05 08:20:26 -0800 2010|\n",
      "|Tue Jul 09 21:53:42 -0700 2013|\n",
      "|Sun Jan 01 23:17:28 -0800 2017|\n",
      "+------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Show a sample of the date_added column\n",
    "df.select(\"date_added\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3743bf5d-3ca7-4852-9909-9323bc5b0480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# covnert date_added to iso format and name it date_added_iso and then change its data type to date\n",
    "from pyspark.sql.functions import regexp_extract, concat_ws, col\n",
    "\n",
    "# Extract year, month, day using regex\n",
    "df = df.withColumn(\"year\", regexp_extract(col(\"date_added\"), r\"\\d{4}$\", 0)) \\\n",
    "       .withColumn(\"month_str\", regexp_extract(col(\"date_added\"), r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b\", 0)) \\\n",
    "       .withColumn(\"day\", regexp_extract(col(\"date_added\"), r\"\\b\\d{2}\\b\", 0))\n",
    "\n",
    "# Map month names to numbers\n",
    "month_dict = {\n",
    "    \"Jan\":\"01\",\"Feb\":\"02\",\"Mar\":\"03\",\"Apr\":\"04\",\"May\":\"05\",\"Jun\":\"06\",\n",
    "    \"Jul\":\"07\",\"Aug\":\"08\",\"Sep\":\"09\",\"Oct\":\"10\",\"Nov\":\"11\",\"Dec\":\"12\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1506be9-89a0-48a1-87d9-5a2af72052c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------+\n",
      "|date_added                    |date_added_iso|\n",
      "+------------------------------+--------------+\n",
      "|Tue Apr 07 16:19:43 -0700 2015|2015-04-07    |\n",
      "|Sun Jun 01 21:34:59 -0700 2014|2014-06-01    |\n",
      "|Tue Jan 05 08:20:26 -0800 2010|2010-01-05    |\n",
      "|Tue Jul 09 21:53:42 -0700 2013|2013-07-09    |\n",
      "|Sun Jan 01 23:17:28 -0800 2017|2017-01-01    |\n",
      "+------------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- n_votes: integer (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- date_added_iso: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*month_dict.items())])\n",
    "df = df.withColumn(\"month\", mapping_expr[col(\"month_str\")])\n",
    "\n",
    "# Combine into yyyy-mm-dd and cast to date\n",
    "df = df.withColumn(\"date_added_iso\", concat_ws(\"-\", col(\"year\"), col(\"month\"), col(\"day\")).cast(\"date\"))\n",
    "\n",
    "# Drop intermediate columns if you want\n",
    "df = df.drop(\"year\", \"month_str\", \"day\", \"month\")\n",
    "\n",
    "# Show result\n",
    "df.select(\"date_added\", \"date_added_iso\").show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a152c542-9def-4d2d-ab21-9cab9691ed32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE cleaning: 14971371 rows\n"
     ]
    }
   ],
   "source": [
    "before_count = df.count()\n",
    "print(f\"BEFORE cleaning: {before_count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0695845-d7eb-4247-99f6-0df781aa9d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER removing NULLs: 14971371 rows\n",
      "AFTER removing empty strings: 14971371 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "# Remove nulls\n",
    "df = df.dropna(subset=[\"rating\", \"book_id\", \"review_text\", \"author_id\"])\n",
    "after_null = df.count()\n",
    "print(f\"AFTER removing NULLs: {after_null} rows\")\n",
    "\n",
    "# Remove EMPTY STRINGS\n",
    "df = df.filter(\n",
    "    (col(\"rating\").isNotNull()) &\n",
    "    (trim(col(\"book_id\")) != \"\") &\n",
    "    (trim(col(\"review_text\")) != \"\") &\n",
    "    (trim(col(\"author_id\")) != \"\")\n",
    ")\n",
    "after_empty = df.count()\n",
    "print(f\"AFTER removing empty strings: {after_empty} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be3ae6b-b275-4ea1-8f6d-af6aabe4246c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER removing review_id duplicates: 14971371 rows\n",
      "AFTER removing (user_id, book_id) duplicates: 14971371 rows\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "df = df.dropDuplicates([\"review_id\"])\n",
    "after_review_id = df.count()\n",
    "print(f\"AFTER removing review_id duplicates: {after_review_id} rows\")\n",
    "\n",
    "df = df.dropDuplicates([\"user_id\", \"book_id\"])\n",
    "after_user_book = df.count()\n",
    "print(f\"AFTER removing (user_id, book_id) duplicates: {after_user_book} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9338b4d2-3191-4c52-b590-036363b4f5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trim text fields\n",
    "df = df.withColumn(\"title\", trim(col(\"title\"))) \\\n",
    "       .withColumn(\"name\", trim(col(\"name\"))) \\\n",
    "       .withColumn(\"review_text\", trim(col(\"review_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea40a1d-112d-4217-87ef-8ea3bf1b3d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE CHECK ===\n",
      "Total rows: 14971371\n",
      "Unique review_ids: 14971371\n",
      "Unique (user_id, book_id): 14971371\n",
      "Duplicate review_ids: 0\n",
      "Duplicate (user, book) pairs: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates BEFORE removing\n",
    "print(\"=== DUPLICATE CHECK ===\")\n",
    "total = df.count()\n",
    "unique_review_ids = df.select(\"review_id\").distinct().count()\n",
    "unique_user_book = df.select(\"user_id\", \"book_id\").distinct().count()\n",
    "\n",
    "print(f\"Total rows: {total}\")\n",
    "print(f\"Unique review_ids: {unique_review_ids}\")\n",
    "print(f\"Unique (user_id, book_id): {unique_user_book}\")\n",
    "print(f\"Duplicate review_ids: {total - unique_review_ids}\")\n",
    "print(f\"Duplicate (user, book) pairs: {total - unique_user_book}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31b6b57-13e8-4f74-9d5f-c4e867bec9da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|review_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|this is a funny, funny, worldless book about a fox that steals a chicken and her animal friends' search to find her. i won't give away the unexpected ending, but it will crack you up! great for sharing with small groups of children (no more than 5), and for encouraging beginning readers/writers to come up with their own words.                                                                                                                                                            |\n",
      "|so, i had to jump on the crossover madness, and even though i've often been a fan of bendis, there was just too much going on for me to care.  i was slightly interested in the phoenix storyline because i've never read any of the previous incarnations, but it was such a mess that i wondered what i was doing in this little sidequest of a comic. i could have been reading hulk or more wolverine-prof-x, both of which i was really getting into. call this a learning experience. move on.|\n",
      "|loved this books so much                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|i'll stop at that.. i just can't, read it for the hype but was kinda disappointed, tho i won't lie, i enjoyed some parts, i loved simon he was my favorite *.*, and of course magnus bane and alec <3 .  this book was the best so far, although i couldn't help but feel like the first two should have been combined into a one book.                                                                                                                                                             |\n",
      "|hmm. i think the reason why i didn't like this so much was because it was more aimed at middle schoolers. the writing was a little young for me, and i would have enjoyed it much more a few years ago.  \"life isn't something that happens to you. it's something you make yourself, all the time.\"                                                                                                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "# Normalize review_text\n",
    "\n",
    "df = df.withColumn(\"review_text\", lower(col(\"review_text\"))) \\\n",
    "       .withColumn(\"review_text\", regexp_replace(col(\"review_text\"), \"[^\\\\x20-\\\\x7E]\", \"\"))\n",
    "\n",
    "# Preview result\n",
    "df.select(\"review_text\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ccb150-bb68-4f5c-b5aa-a21b2e3dc9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------+\n",
      "|title                    |name              |\n",
      "+-------------------------+------------------+\n",
      "|The Chicken Thief        |Rose Vanden Eynden|\n",
      "|Avengers Vs. X-men       |Toby Tate         |\n",
      "|Schmetterlinge Im Dunkeln|Lois Kapila       |\n",
      "|The Book Thief           |Essi Kummu        |\n",
      "|Two By Two               |Siim Veskimees    |\n",
      "+-------------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap, col\n",
    "\n",
    "# Capitalize each word in title and name\n",
    "df = df.withColumn(\"title\", initcap(col(\"title\"))) \\\n",
    "       .withColumn(\"name\", initcap(col(\"name\")))\n",
    "\n",
    "# Preview result\n",
    "df.select(\"title\", \"name\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccee81da-0189-4a9a-9b0a-c0adfd2c435e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|review_length|\n",
      "+-------------+\n",
      "|256          |\n",
      "|889          |\n",
      "|169          |\n",
      "|27           |\n",
      "|3426         |\n",
      "+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create review lenght column to count number of characters in review\n",
    "from pyspark.sql.functions import length, col\n",
    "df = df.withColumn(\"review_length\", length(col(\"review_text\")))\n",
    "\n",
    "df.select(\"review_length\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2ce538-b43e-47c0-92d7-e601987a6a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 14971313\n"
     ]
    }
   ],
   "source": [
    "before_count = df.count()\n",
    "df = df.filter(col(\"review_length\") >= 10)\n",
    "after_count = df.count()\n",
    "print(f\"Number of rows after filtering: {after_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31dfee89-294c-4b4b-a3ad-25db18fbc109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_date\n",
    "# Remove rows with invalid or future dates\n",
    "df = df.filter(\n",
    "    (col(\"date_added_iso\").isNotNull()) &\n",
    "    (col(\"date_added_iso\") <= current_date())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e15625e-c160-4f52-a2cc-0cd2e3b8886a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# Replacing missing n_votes with value 0\n",
    "df = df.fillna({\"n_votes\": 0})\n",
    "\n",
    "# Replace null or empty language_code with \"Unknown\"\n",
    "df = df.withColumn(\n",
    "    \"language_code\",\n",
    "    when((col(\"language_code\").isNull()) | (col(\"language_code\") == \"\"), \"Unknown\")\n",
    "    .otherwise(col(\"language_code\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b160bc-c68f-434f-8fe9-aba73dcc91ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'review_id' has 0 null values\n",
      "Column 'book_id' has 0 null values\n",
      "Column 'title' has 0 null values\n",
      "Column 'author_id' has 0 null values\n",
      "Column 'name' has 0 null values\n",
      "Column 'user_id' has 0 null values\n",
      "Column 'rating' has 0 null values\n",
      "Column 'review_text' has 0 null values\n",
      "Column 'language_code' has 0 null values\n",
      "Column 'n_votes' has 0 null values\n",
      "Column 'date_added' has 0 null values\n",
      "Column 'date_added_iso' has 0 null values\n",
      "Column 'review_length' has 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Loop through all columns and count nulls and Verify that all columns contain valid values\n",
    "for c in df.columns:\n",
    "    null_count = df.filter(col(c).isNull()).count()\n",
    "    print(f\"Column '{c}' has {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa856ee-a991-4cc1-949e-1de04a79bac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'title' has 6 empty strings\n",
      "Column 'name' has 82 empty strings\n",
      "Column 'review_text' has 0 empty strings\n",
      "Column 'language_code' has 0 empty strings\n"
     ]
    }
   ],
   "source": [
    "text_cols = [\"title\", \"name\", \"review_text\", \"language_code\"]\n",
    "for c in text_cols:\n",
    "    empty_count = df.filter(col(c) == \"\").count()\n",
    "    print(f\"Column '{c}' has {empty_count} empty strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86385f0-b32f-49bc-88c0-1bf31c250779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty title/name: 14971313\n",
      "After removing empty title/name: 14971225\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with empty title or name\n",
    "print(f\"Before removing empty title/name: {df.count()}\")\n",
    "df = df.filter((col(\"title\") != \"\") & (col(\"name\") != \"\"))\n",
    "print(f\"After removing empty title/name: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f198a39-e6c5-4fcc-8ace-88be3890db65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- n_votes: integer (nullable = false)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- date_added_iso: date (nullable = true)\n",
      " |-- review_length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show schema to verify types and check column names\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36dd2493-bff3-4bb0-98e1-e94f03f5dd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_id',\n",
       " 'book_id',\n",
       " 'title',\n",
       " 'author_id',\n",
       " 'name',\n",
       " 'user_id',\n",
       " 'rating',\n",
       " 'review_text',\n",
       " 'language_code',\n",
       " 'n_votes',\n",
       " 'date_added_iso']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the unnecessary columns\n",
    "df = df.drop(\"date_added\", \"review_length\")\n",
    "\n",
    "# Check remaining columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b800f5-a6a7-438c-92c1-839ca63bfdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid ratings: 0\n",
      "Negative votes: 1523\n"
     ]
    }
   ],
   "source": [
    "#Verify that all numeric columns contain valid values and within expected ranges\n",
    "# Check RATING (1-5)\n",
    "invalid_rating = df.filter((col(\"rating\") < 1) | (col(\"rating\") > 5)).count()\n",
    "print(f\"Invalid ratings: {invalid_rating}\")\n",
    "\n",
    "# Negative votes might represent downvotes or \"not helpful\" votes\n",
    "invalid_votes = df.filter(col(\"n_votes\") < 0).count()\n",
    "print(f\"Negative votes: {invalid_votes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a442ccd4-cccb-4ae0-8a4a-872ed073653b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|         review_text|review_word_count|\n",
      "+--------------------+-----------------+\n",
      "|i kept waiting fo...|               46|\n",
      "|despite a slow st...|               34|\n",
      "|this was the perf...|               76|\n",
      "|well...i expected...|               43|\n",
      "|review also poste...|              312|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add Aggregate columns\n",
    "# Compute review length in words\n",
    "from pyspark.sql.functions import length, split, size, col\n",
    "df = df.withColumn('review_word_count', size(split(col('review_text'), ' ')))\n",
    "df.select('review_text', 'review_word_count').show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1437ba-af3b-4eab-beeb-1093aa7fb197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+\n",
      "| book_id|avg_book_rating|num_reviews_per_book|\n",
      "+--------+---------------+--------------------+\n",
      "|15904647|            4.0|                   3|\n",
      "|20453985|           3.88|                  74|\n",
      "|13539044|            4.0|                1786|\n",
      "|10837174|           4.12|                  73|\n",
      "|26185593|            3.0|                   1|\n",
      "+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, round\n",
    "# Group by book_id and calculate aggregates\n",
    "book_features = (\n",
    "    df.groupBy(\"book_id\")\n",
    "      .agg(\n",
    "          round(avg(\"rating\"), 2).alias(\"avg_book_rating\"),\n",
    "          count(\"review_id\").alias(\"num_reviews_per_book\")\n",
    "      )\n",
    ")\n",
    "# Show first few rows\n",
    "book_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776414d0-fe3b-4c1f-9427-49b5fa768b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|             name|author_avg_rating|\n",
      "+-----------------+-----------------+\n",
      "|Heickmann, Werner|             4.04|\n",
      "|    J.m. Franklin|             4.25|\n",
      "|      Kyle Nelson|             4.41|\n",
      "| Phyllis Mcginley|             3.85|\n",
      "|       Jeff Koons|              4.1|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Average rating per Author\n",
    "author_avg = (\n",
    "    df.groupBy(\"name\")\n",
    "      .agg(\n",
    "          round(avg(\"rating\"), 2).alias(\"author_avg_rating\")\n",
    "      )\n",
    ")\n",
    "author_avg.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78cab786-445a-4004-bfd1-d85de26908f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+------------------+\n",
      "| book_id|min_words_per_book|max_words_per_book|avg_words_per_book|\n",
      "+--------+------------------+------------------+------------------+\n",
      "|12874448|                 3|               460|            100.08|\n",
      "|18335634|                 1|              2062|             107.4|\n",
      "|28954126|                 1|               902|             98.25|\n",
      "| 7937843|                 1|              3548|            101.42|\n",
      "|29371079|                 4|               779|            266.36|\n",
      "+--------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, round, min as spark_min, max as spark_max, col, size, split\n",
    "\n",
    "word_stats_per_book = df.groupBy('book_id').agg(\n",
    "    spark_min('review_word_count').alias('min_words_per_book'),\n",
    "    spark_max('review_word_count').alias('max_words_per_book'),\n",
    "    round(avg('review_word_count'), 2).alias('avg_words_per_book')\n",
    ")\n",
    "\n",
    "word_stats_per_book.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea54130-5d46-4a83-921a-a8021515c176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join book-level features (avg rating & num reviews)\n",
    "df = df.join(book_features, on='book_id', how='left')\n",
    "\n",
    "# Join author-level average rating\n",
    "df = df.join(author_avg, on='name', how='left')\n",
    "\n",
    "# Join word count statistics per book\n",
    "df = df.join(word_stats_per_book, on='book_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aff7f43-bd7c-4ca3-9a8f-1d45b1592e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop helper column\n",
    "df = df.drop('review_word_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a379a7b4-0f3f-4c8f-b805-9a61b92cc4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"num_reviews_per_book\", df[\"num_reviews_per_book\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a597e1-d374-409e-9705-615111b37449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Replace empty title/name with \"Unknown\"\n",
    "# df = df.withColumn(\"title\", when(col(\"title\") == \"\", \"Unknown\").otherwise(col(\"title\")))\n",
    "# df = df.withColumn(\"name\", when(col(\"name\") == \"\", \"Unknown Author\").otherwise(col(\"name\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6090db-8967-41f6-a2ee-e295a2f0286e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFYING N_VOTES RANGE ===\n",
      "Negative n_votes found: 1523\n",
      "Before removing negative n_votes: 14971225\n",
      "After removing negative n_votes: 14969702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Negative n_votes found: {df.filter(col('n_votes') < 0).count()}\")\n",
    "\n",
    "# Remove negative n_votes (invalid data range)\n",
    "print(f\"Before removing negative n_votes: {df.count()}\")\n",
    "df = df.filter(col(\"n_votes\") >= 0)\n",
    "print(f\"After removing negative n_votes: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13722c60-6057-42fc-bf99-48f341e93e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns: 17\n",
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- n_votes: integer (nullable = false)\n",
      " |-- date_added_iso: date (nullable = true)\n",
      " |-- avg_book_rating: double (nullable = true)\n",
      " |-- num_reviews_per_book: integer (nullable = true)\n",
      " |-- author_avg_rating: double (nullable = true)\n",
      " |-- min_words_per_book: integer (nullable = true)\n",
      " |-- max_words_per_book: integer (nullable = true)\n",
      " |-- avg_words_per_book: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final columns: {len(df.columns)}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573ab09a-640c-4d0d-8ba9-2ce4a695b8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Gold layer path where the Delta table will be stored\n",
    "gold_path_v2  = \"abfss://lakehouse@goodreadsreviews60104758.dfs.core.windows.net/Gold_layer/features_v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cf9f9d-0f08-435f-a1c2-9a0fdc9f7b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Delta table in the Gold path\n",
    "# mode='overwrite' ensures that if the table already exists, it will be replaced\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(gold_path_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f953aeaf-4802-4627-8184-a1b0f14231a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------------+--------------------+---------+--------------------+------+--------------------+-------------+-------+--------------+---------------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "| book_id|           name|           review_id|               title|author_id|             user_id|rating|         review_text|language_code|n_votes|date_added_iso|avg_book_rating|num_reviews_per_book|author_avg_rating|min_words_per_book|max_words_per_book|avg_words_per_book|\n",
      "+--------+---------------+--------------------+--------------------+---------+--------------------+------+--------------------+-------------+-------+--------------+---------------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "|12727768|   Winsor Mccay|fb25df0002eac4f9f...|Open Minds (mindj...|   224293|babc631ce1f8d2dea...|     2|2.5 stars  kira m...|          eng|      0|    2012-10-14|           4.03|                 269|             4.03|                 1|               917|            178.25|\n",
      "|      11| Annie Wedekind|a7342a32d1ab748dc...|The Hitchhiker's ...|  1246638|fcb738986a527d5a8...|     5|douglas adams bro...|        en-US|      0|    2014-05-22|           4.02|                2652|             4.02|                 1|              3347|             70.15|\n",
      "|   27266|  Susan   Marie|a95afde0aba899654...|Glass Houses (the...|  6550302|e8d86263b0fd61118...|     4|i really enjoyed ...|        en-US|      0|    2008-10-24|           3.68|                 979|              3.7|                 1|              1503|            118.93|\n",
      "| 7060582|Jessica Meserve|622e0319d2295dfe7...|Twice Bitten (chi...|   465626|248e958446e1905d4...|     5|i am in love with...|          eng|      0|    2013-01-22|           4.35|                 455|             4.35|                 1|              1950|            138.81|\n",
      "|11870085|  Gerard Meudal|6279da447852fc203...|The Fault In Our ...|   375874|8684d1cd480bb3681...|     4|it's not like thi...|          eng|      0|    2014-02-12|           4.41|               20070|             4.41|                 1|              3104|            104.96|\n",
      "+--------+---------------+--------------------+--------------------+---------+--------------------+------+--------------------+-------------+-------+--------------+---------------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Register the Delta table in the Databricks metastore for SQL queries\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Gold_layer.features_v1\n",
    "USING DELTA\n",
    "LOCATION '{gold_path_v2}'\n",
    "\"\"\")\n",
    "\n",
    "# Show a few rows to verify the saved dataset\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6511581903968172,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
